{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection\n",
    "\n",
    "Write a software pipeline to identify vehicles in a video from a front-facing camera on a car."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step - 0: Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from glob import glob\n",
    "from random import randint\n",
    "from random import shuffle\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from skimage.feature import hog\n",
    "from scipy import ndimage\n",
    "from scipy.ndimage.measurements import label\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "# sklearn\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "# keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten,Lambda\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step - 1: Feature Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Read in  `vehicle` and `non-vehicle` images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.1 object-detection-crowdai**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_1 = pd.read_csv('object-detection-crowdai/labels.csv', header=0)\n",
    "data_1['File_Path'] = 'object-detection-crowdai/' + data_1['Frame']\n",
    "data_1 = data_1.drop('Preview URL', 1)\n",
    "data_1 = data_1.drop('Frame', 1)\n",
    "data_1['Label'] = data_1['Label'].str.lower()\n",
    "data_1.columns = ['xmin','ymin','xmax','ymax', 'Label', 'File_Path']\n",
    "data_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Summaries\n",
    "print(\"Number of examples =\", len(data_1))\n",
    "print(\"Unique lables are\", np.unique(data_1['Label']))\n",
    "print(\"Number of cars =\", len(data_1[data_1['Label']=='car']))\n",
    "print(\"Number of trucks =\", len(data_1[data_1['Label']=='truck']))\n",
    "print(\"Number of pedestrians =\", len(data_1[data_1['Label']=='pedestrian']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.2 object-dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "names = ['Frame', 'xmin','ymin','xmax','ymax', 'ind', 'Label','RM']\n",
    "data_2 = pd.read_csv('object-dataset/labels.csv', delim_whitespace=True, names=names)\n",
    "data_2['File_Path'] = 'object-dataset/' + data_2['Frame']\n",
    "data_2 = data_2.drop('RM', 1)\n",
    "data_2 = data_2.drop('Frame', 1)\n",
    "data_2 = data_2.drop('ind', 1)\n",
    "data_2.columns = ['xmin','ymin','xmax','ymax', 'Label', 'File_Path']\n",
    "data_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summaries\n",
    "print(\"Number of examples =\", len(data_2))\n",
    "print(\"Unique lables are\", np.unique(data_2['Label']))\n",
    "print(\"Number of cars =\", len(data_2[data_2['Label']=='car']))\n",
    "print(\"Number of trucks =\", len(data_2[data_2['Label']=='truck']))\n",
    "print(\"Number of pedestrians =\", len(data_2[data_2['Label']=='pedestrian']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.3 Combine object-detection-crowdai and object-dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_udacity = pd.concat([data_1, data_2]).reset_index()\n",
    "data_udacity = data_udacity.drop('index', 1)\n",
    "data_udacity.columns = ['xmin','ymin','xmax','ymax', 'Label', 'File_Path']\n",
    "data_udacity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summaries\n",
    "print(\"Number of examples =\", len(data_udacity))\n",
    "print(\"Unique lables are\", np.unique(data_udacity['Label']))\n",
    "print(\"Number of cars =\", len(data_udacity[data_udacity['Label']=='car']))\n",
    "print(\"Number of trucks =\", len(data_udacity[data_udacity['Label']=='truck']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.4.1 Sample images from the udacity dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check number of examples\n",
    "d_cars = data_udacity[(data_udacity['Label'] == 'car') | (data_udacity['Label'] == 'truck')].reset_index()\n",
    "d_notcars = data_udacity[(data_udacity['Label'] != 'car') & (data_udacity['Label'] != 'truck')].reset_index()\n",
    "\n",
    "print(\"Number of examples =\", len(data_udacity)) \n",
    "print(\"Number of car examples =\", len(d_cars)) \n",
    "print(\"Number of other examples =\", len(d_notcars))                                                             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "# Sample 3000 each from data_udacity\n",
    "sample = 3000\n",
    "\n",
    "for i in range(sample):\n",
    "    index = randint(0, len(d_cars)-1)\n",
    "    image = cv2.imread(d_cars['File_Path'][index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image[d_cars['ymin'][index]:d_cars['ymax'][index], \n",
    "                  d_cars['xmin'][index]:d_cars['xmax'][index]]\n",
    "    image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    cars.append(image)\n",
    "\n",
    "for i in range(sample):\n",
    "    index = randint(0, len(d_notcars)-1)\n",
    "    image = cv2.imread(d_notcars['File_Path'][index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image[d_notcars['ymin'][index]:d_notcars['ymax'][index], \n",
    "                  d_notcars['xmin'][index]:d_notcars['xmax'][index]]\n",
    "    image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    notcars.append(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summaries\n",
    "print(\"Number of car examples =\", len(cars)) \n",
    "print(\"Number of other examples =\", len(notcars)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.4.2 Sample train and test images separately from the two datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use data_2 for train and data_1 for test\n",
    "d_cars_train = data_2[(data_2['Label'] == 'car') | (data_2['Label'] == 'truck')].reset_index()\n",
    "print(\"d_cars_train - \", len(d_cars_train))\n",
    "\n",
    "d_cars_train = d_cars_train[(d_cars_train['xmax'] - d_cars_train['xmin'] > 0) &\n",
    "                            (d_cars_train['ymax'] - d_cars_train['ymin'] > 0)].reset_index()\n",
    "print(\"d_cars_train (after) - \", len(d_cars_train))\n",
    "\n",
    "d_cars_test = data_1[(data_1['Label'] == 'car') | (data_1['Label'] == 'truck')].reset_index()\n",
    "print(\"d_cars_test - \", len(d_cars_test))\n",
    "\n",
    "d_cars_test = d_cars_test[(d_cars_test['xmax'] - d_cars_test['xmin'] > 0) &\n",
    "                            (d_cars_test['ymax'] - d_cars_test['ymin'] > 0)].reset_index()\n",
    "print(\"d_cars_test (after) - \", len(d_cars_test))\n",
    "\n",
    "d_notcars_train= data_2[(data_2['Label'] != 'car') & (data_2['Label'] != 'truck')].reset_index()\n",
    "print(\"d_notcars_train - \", len(d_notcars_train))\n",
    "\n",
    "d_notcars_train = d_notcars_train[(d_notcars_train['xmax'] - d_notcars_train['xmin'] > 0) &\n",
    "                            (d_notcars_train['ymax'] - d_notcars_train['ymin'] > 0)].reset_index()\n",
    "print(\"d_notcars_train (after) - \", len(d_notcars_train))\n",
    "\n",
    "d_notcars_test = data_1[(data_1['Label'] != 'car') & (data_1['Label'] != 'truck')].reset_index()\n",
    "print(\"d_notcars_test - \", len(d_notcars_test))\n",
    "\n",
    "d_notcars_test = d_notcars_test[(d_notcars_test['xmax'] - d_notcars_test['xmin'] > 0) &\n",
    "                            (d_notcars_test['ymax'] - d_notcars_test['ymin'] > 0)].reset_index()\n",
    "print(\"d_notcars_test (after) - \", len(d_notcars_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cars_train = []\n",
    "cars_test = []\n",
    "notcars_train = []\n",
    "notcars_test = []\n",
    "\n",
    "# Another 2000 images will be added to training set from KITTI and GTI sources\n",
    "sample_train = 8000\n",
    "sample_test = 2000\n",
    "\n",
    "# Sample training data\n",
    "for i in range(sample_train):\n",
    "    index = randint(0, len(d_cars_train)-1)\n",
    "    image = cv2.imread(d_cars_train['File_Path'][index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image[d_cars_train['ymin'][index]:d_cars_train['ymax'][index], \n",
    "                  d_cars_train['xmin'][index]:d_cars_train['xmax'][index]]\n",
    "    image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    cars_train.append(image)\n",
    "\n",
    "for i in range(sample_train):\n",
    "    index = randint(0, len(d_notcars_train)-1)\n",
    "    image = cv2.imread(d_notcars_train['File_Path'][index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image[d_notcars_train['ymin'][index]:d_notcars_train['ymax'][index], \n",
    "                  d_notcars_train['xmin'][index]:d_notcars_train['xmax'][index]]\n",
    "    image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    notcars_train.append(image)    \n",
    "\n",
    "# Sample testing data\n",
    "for i in range(sample_test):\n",
    "    index = randint(0, len(d_cars_test)-1)\n",
    "    image = cv2.imread(d_cars_test['File_Path'][index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image[d_cars_test['ymin'][index]:d_cars_test['ymax'][index], \n",
    "                  d_cars_test['xmin'][index]:d_cars_test['xmax'][index]]\n",
    "    image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    cars_test.append(image)\n",
    "\n",
    "for i in range(sample_test):\n",
    "    index = randint(0, len(d_notcars_test)-1)\n",
    "    image = cv2.imread(d_notcars_test['File_Path'][index])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image = image[d_notcars_test['ymin'][index]:d_notcars_test['ymax'][index], \n",
    "                  d_notcars_test['xmin'][index]:d_notcars_test['xmax'][index]]\n",
    "    image = cv2.resize(image, (64, 64), interpolation=cv2.INTER_AREA)\n",
    "    notcars_test.append(image)    \n",
    "    \n",
    "# Summaries\n",
    "print(\"Number of car train examples =\", len(cars_train)) \n",
    "print(\"Number of other train examples =\", len(notcars_train)) \n",
    "print(\"Number of car test examples =\", len(cars_test)) \n",
    "print(\"Number of other test examples =\", len(notcars_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.5.1 Sample images from vehicle and non-vehicles folder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of images in each folder\n",
    "print (\"KITTI_extracted:\", len([img for img in os.listdir('vehicles/KITTI_extracted')]))\n",
    "print (\"GTI_Far:\", len([img for img in os.listdir('vehicles/GTI_Far')]))\n",
    "print (\"GTI_Left:\", len([img for img in os.listdir('vehicles/GTI_Left')]))\n",
    "print (\"GTI_MiddleClose:\", len([img for img in os.listdir('vehicles/GTI_MiddleClose')]))\n",
    "print (\"GTI_Right:\", len([img for img in os.listdir('vehicles/GTI_Right')]))\n",
    "print (\"non-vehicles/GTI:\", len([img for img in os.listdir('non-vehicles/GTI')]))\n",
    "print (\"non-vehicles/Extras:\", len([img for img in os.listdir('non-vehicles/Extras')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample and append to cars and notcars data\n",
    "\n",
    "# Sample 1000 cars\n",
    "files = glob(\"vehicles/KITTI_extracted/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(600):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars.append(image)\n",
    "\n",
    "files = glob(\"vehicles/GTI_Far/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(100):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars.append(image)\n",
    "\n",
    "files = glob(\"vehicles/GTI_Left/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(100):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars.append(image)\n",
    "\n",
    "files = glob(\"vehicles/GTI_MiddleClose/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(100):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars.append(image)\n",
    "    \n",
    "files = glob(\"vehicles/GTI_Right/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(100):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars.append(image)\n",
    "    \n",
    "# Sample 1000 notcars\n",
    "files = glob(\"non-vehicles/GTI/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(400):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    notcars.append(image)\n",
    "\n",
    "files = glob(\"non-vehicles/Extras/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(600):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    notcars.append(image)\n",
    "\n",
    "# Summaries    \n",
    "print(\"Number of car examples =\", len(cars)) \n",
    "print(\"Number of other examples =\", len(notcars)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at few images\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(cars[101])  \n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(notcars[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.5.2 Sample images from vehicle and non-vehicles folder but only append to train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample and append to cars_train and notcars_train data\n",
    "\n",
    "# Sample 2000 cars\n",
    "files = glob(\"vehicles/KITTI_extracted/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(1200):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars_train.append(image)\n",
    "\n",
    "files = glob(\"vehicles/GTI_Far/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(200):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars_train.append(image)\n",
    "\n",
    "files = glob(\"vehicles/GTI_Left/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(200):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars_train.append(image)\n",
    "\n",
    "files = glob(\"vehicles/GTI_MiddleClose/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(200):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars_train.append(image)\n",
    "    \n",
    "files = glob(\"vehicles/GTI_Right/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(200):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    cars_train.append(image)\n",
    "    \n",
    "# Sample 2000 notcars\n",
    "files = glob(\"non-vehicles/GTI/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(800):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    notcars_train.append(image)\n",
    "\n",
    "files = glob(\"non-vehicles/Extras/*.png\")\n",
    "shuffle(files)\n",
    "for i in range(1200):\n",
    "    image = cv2.imread(files[i])\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    notcars_train.append(image)\n",
    "\n",
    "# Summaries\n",
    "print(\"Number of car train examples =\", len(cars_train)) \n",
    "print(\"Number of other train examples =\", len(notcars_train)) \n",
    "print(\"Number of car test examples =\", len(cars_test)) \n",
    "print(\"Number of other test examples =\", len(notcars_test)) \n",
    "\n",
    "del data_1, data_2, d_cars_train, d_cars_test, d_notcars_train, d_notcars_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Look at few images\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(cars_train[101])  \n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(notcars_train[101])\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(cars_test[101])  \n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(notcars_test[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Extract color features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b.1 Color histogram features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    # Compute the histogram of the RGB channels separately\n",
    "    rhist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    ghist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    bhist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    # Generating bin centers\n",
    "    bin_edges = rhist[1]\n",
    "    bin_centers = (bin_edges[1:]  + bin_edges[0:len(bin_edges)-1])/2\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((rhist[0], ghist[0], bhist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return rhist, ghist, bhist, bin_centers, hist_features\n",
    "\n",
    "# Check\n",
    "rh, gh, bh, bincen, feature_vec = color_hist(cars_train[101], nbins=32, bins_range=(0, 256))\n",
    "\n",
    "fig = plt.figure(figsize=(12,3))\n",
    "plt.subplot(131)\n",
    "plt.bar(bincen, rh[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('R Histogram')\n",
    "plt.subplot(132)\n",
    "plt.bar(bincen, gh[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('G Histogram')\n",
    "plt.subplot(133)\n",
    "plt.bar(bincen, bh[0])\n",
    "plt.xlim(0, 256)\n",
    "plt.title('B Histogram')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b.2 Explore color spaces**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot3d(pixels, colors_rgb,\n",
    "        axis_labels=list(\"RGB\"), axis_limits=[(0, 255), (0, 255), (0, 255)]):\n",
    "    \"\"\"Plot pixels in 3D.\"\"\"\n",
    "\n",
    "    # Create figure and 3D axes\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = Axes3D(fig)\n",
    "\n",
    "    # Set axis limits\n",
    "    ax.set_xlim(*axis_limits[0])\n",
    "    ax.set_ylim(*axis_limits[1])\n",
    "    ax.set_zlim(*axis_limits[2])\n",
    "\n",
    "    # Set axis labels and sizes\n",
    "    ax.tick_params(axis='both', which='major', labelsize=14, pad=8)\n",
    "    ax.set_xlabel(axis_labels[0], fontsize=16, labelpad=16)\n",
    "    ax.set_ylabel(axis_labels[1], fontsize=16, labelpad=16)\n",
    "    ax.set_zlabel(axis_labels[2], fontsize=16, labelpad=16)\n",
    "\n",
    "    # Plot pixel values with colors given in colors_rgb\n",
    "    ax.scatter(\n",
    "        pixels[:, :, 0].ravel(),\n",
    "        pixels[:, :, 1].ravel(),\n",
    "        pixels[:, :, 2].ravel(),\n",
    "        c=colors_rgb.reshape((-1, 3)), edgecolors='none')\n",
    "\n",
    "    return ax  # return Axes3D object for further manipulation\n",
    "\n",
    "# Convert image to desired color space(s)\n",
    "img_small_RGB = cars_train[101]\n",
    "img_small_HSV = cv2.cvtColor(img_small_RGB, cv2.COLOR_RGB2HSV)\n",
    "img_small_YCrCb = cv2.cvtColor(img_small_RGB, cv2.COLOR_RGB2YCrCb)\n",
    "img_small_rgb = img_small_RGB / 255.  # scaled to [0, 1], only for plotting\n",
    "\n",
    "# Plot and show\n",
    "plot3d(img_small_RGB, img_small_rgb)\n",
    "plot3d(img_small_HSV, img_small_rgb, axis_labels=list(\"HSV\"))\n",
    "plot3d(img_small_YCrCb, img_small_rgb, axis_labels=list(\"Yrb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b.3 Spatial binning of color**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bin_spatial(img, color_space='RGB', size=(32, 32)):\n",
    "    # Convert image to new color space (if specified)\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)             \n",
    "    # Use cv2.resize().ravel() to create the feature vector\n",
    "    features = cv2.resize(feature_image, size).ravel() \n",
    "    # Return the feature vector\n",
    "    return features\n",
    "\n",
    "# Check \n",
    "features = bin_spatial(cars_train[101], color_space='RGB', size=(32, 32))\n",
    "print (features.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Extracted HOG features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **c.1 scikit-image HOG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=True, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=True, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check HOG features\n",
    "image = cv2.cvtColor(cars_train[101], cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "features, hog_image1 = get_hog_features(image, orient=9, pix_per_cell=8, cell_per_block=2, vis=True, feature_vec=False)\n",
    "features, hog_image2 = get_hog_features(image, orient=6, pix_per_cell=4, cell_per_block=2, vis=True, feature_vec=False)\n",
    "features, hog_image3 = get_hog_features(image, orient=12, pix_per_cell=8, cell_per_block=2, vis=True, feature_vec=False)\n",
    "features, hog_image4 = get_hog_features(image, orient=9, pix_per_cell=8, cell_per_block=8, vis=True, feature_vec=False)\n",
    "\n",
    "# Plot the examples\n",
    "fig = plt.figure()\n",
    "plt.subplot(151)\n",
    "plt.imshow(cars_train[101])\n",
    "plt.title('Car')\n",
    "plt.subplot(152)\n",
    "plt.imshow(hog_image1, cmap='gray')\n",
    "plt.title('(9, 8, 2)')\n",
    "plt.subplot(153)\n",
    "plt.imshow(hog_image1, cmap='gray')\n",
    "plt.title('(6, 4, 2)')\n",
    "plt.subplot(154)\n",
    "plt.imshow(hog_image1, cmap='gray')\n",
    "plt.title('(12, 8, 2)')\n",
    "plt.subplot(155)\n",
    "plt.imshow(hog_image1, cmap='gray')\n",
    "plt.title('(9, 8, 8)')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **c.2 Combine and normalize features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_spatial(img, size=(32, 32)):\n",
    "    features = cv2.resize(img, size).ravel() \n",
    "    return features\n",
    "\n",
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    return hist_features\n",
    "\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32), hist_bins=32, orient=9, \n",
    "                     pix_per_cell=8, cell_per_block=2, hog_channel=\"ALL\",\n",
    "                     spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for image in imgs:\n",
    "        file_features = []\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "        if hog_feat == True:\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        features.append(np.concatenate(file_features))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune the parameters\n",
    "colorspace = 'RGB' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "spatial = 32\n",
    "histbin = 32\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "hog_channel = \"0\" # Can be 0, 1, 2, or \"ALL\"\n",
    "\n",
    "car_features = extract_features(cars_train[101:110], color_space='RGB', spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, hog_channel=hog_channel)\n",
    "\n",
    "notcar_features = extract_features(notcars_train[101:110], color_space='RGB', spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, hog_channel=hog_channel)\n",
    "\n",
    "# Create an array stack of feature vectors\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "# Define the labels vector\n",
    "#y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "# Plot one for car\n",
    "plt.subplot(231)\n",
    "plt.imshow(cars_train[101])\n",
    "plt.title('Car Original Image')\n",
    "plt.subplot(232)\n",
    "plt.plot(X[0])\n",
    "plt.title('Car Raw Features')\n",
    "plt.subplot(233)\n",
    "plt.plot(scaled_X[0])\n",
    "plt.title('Car Normalized Features')\n",
    "\n",
    "# Plot one for notcar\n",
    "plt.subplot(234)\n",
    "plt.imshow(notcars_train[101])\n",
    "plt.title('NotCar Original Image')\n",
    "plt.subplot(235)\n",
    "plt.plot(X[10])\n",
    "plt.title('NotCar Raw Features')\n",
    "plt.subplot(236)\n",
    "plt.plot(scaled_X[10])\n",
    "plt.title('NotCar Normalized Features')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step - 2: Build a Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Prep training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.1 Sampling strategy 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune the parameters\n",
    "colorspace = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "spatial = 32\n",
    "histbin = 32\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "\n",
    "car_features = extract_features(cars, color_space=colorspace, spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, hog_channel=hog_channel)\n",
    "\n",
    "notcar_features = extract_features(notcars, color_space=colorspace, spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, hog_channel=hog_channel)\n",
    "\n",
    "X = np.vstack((car_features, notcar_features)).astype(np.float64)                        \n",
    "X_scaler = StandardScaler().fit(X)\n",
    "scaled_X = X_scaler.transform(X)\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))))\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(scaled_X, y, test_size=0.2, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Summaries\n",
    "print(\"Number of training examples =\", len(X_train)) \n",
    "print(\"Number of testing examples =\", len(X_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **a.2 Sampling strategy 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tune the parameters\n",
    "colorspace = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "spatial = 32\n",
    "histbin = 32\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "\n",
    "# Train\n",
    "car_train_features = extract_features(cars_train, color_space=colorspace, spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, hog_channel=hog_channel)\n",
    "\n",
    "notcar_train_features = extract_features(notcars_train, color_space=colorspace, spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, hog_channel=hog_channel)\n",
    "\n",
    "X = np.vstack((car_train_features, notcar_train_features)).astype(np.float64)                        \n",
    "X_scaler = StandardScaler().fit(X)\n",
    "X_train = X_scaler.transform(X)\n",
    "y_train = np.hstack((np.ones(len(car_train_features)), np.zeros(len(notcar_train_features))))\n",
    "\n",
    "# Test\n",
    "car_test_features = extract_features(cars_test, color_space=colorspace, spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, hog_channel=hog_channel)\n",
    "\n",
    "notcar_test_features = extract_features(notcars_test, color_space=colorspace, spatial_size=(spatial, spatial),\n",
    "                        hist_bins=histbin, orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, hog_channel=hog_channel)\n",
    "\n",
    "X_t = np.vstack((car_test_features, notcar_test_features)).astype(np.float64)                        \n",
    "X_test = X_scaler.transform(X_t)\n",
    "y_test = np.hstack((np.ones(len(car_test_features)), np.zeros(len(notcar_test_features))))\n",
    "\n",
    "# Summaries\n",
    "print(\"Number of training examples =\", len(X_train), len(y_train)) \n",
    "print(\"Number of testing examples =\", len(X_test), len(y_test)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b.1 Initial test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test multiple classifiers\n",
    "names = [\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    KNeighborsClassifier(5),\n",
    "    LinearSVC(),\n",
    "    SVC(),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0), warm_start=True),\n",
    "    DecisionTreeClassifier(max_depth=10, min_samples_split=100),\n",
    "    RandomForestClassifier(max_depth=5, min_samples_split=300, n_estimators=25),\n",
    "    MLPClassifier(),\n",
    "    AdaBoostClassifier(learning_rate = 0.01),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()]\n",
    "\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    t = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print('Test Accuracy of', name, '=', clf.score(X_test, y_test), '; cost =', round(t2-t, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b.2 Subset test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test subset multiple classifiers\n",
    "names = [\"Linear SVM\", \"RBF SVM\", \"Random Forest\", \"Neural Net\"]\n",
    "\n",
    "classifiers = [\n",
    "    LinearSVC(),\n",
    "    SVC(),\n",
    "    RandomForestClassifier(max_depth=10, min_samples_split=500, n_estimators=50),\n",
    "    MLPClassifier()]\n",
    "\n",
    "# iterate over classifiers\n",
    "for name, clf in zip(names, classifiers):\n",
    "    t = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    t2 = time.time()\n",
    "    print('Test Accuracy of', name, '=', clf.score(X_test, y_test), '; cost =', round(t2-t, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **b.3 Final classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use Neural Net for further testing\n",
    "clf = MLPClassifier(verbose=True)\n",
    "#clf = LinearSVC()\n",
    "t = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print('Test Accuracy of Neural Net =', clf.score(X_test, y_test), '; cost =', round(t2-t, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. LeNet-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **c.1 Sampling strategy 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize to 0.1 - 0.9\n",
    "image = []\n",
    "cars_small = []\n",
    "notcars_small = []\n",
    "for img in cars:\n",
    "    cars_small.append(cv2.resize(img, (32,32)))\n",
    "for img in notcars:\n",
    "    notcars_small.append(cv2.resize(img, (32,32)))  \n",
    "image = np.append(cars_small, notcars_small, axis=0)\n",
    "image_normalize = image / 255 * 0.8 + 0.1\n",
    "lable = np.hstack((np.ones(len(cars)), np.zeros(len(notcars))))\n",
    "\n",
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_test, y_train, y_test = train_test_split(image_normalize, lable, test_size=0.2, random_state=rand_state)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.2, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **c.2 Sampling strategy 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Normalize to 0.1 - 0.9\n",
    "train = []\n",
    "cars_train_small = []\n",
    "notcars_train_small = []\n",
    "test = []\n",
    "cars_test_small = []\n",
    "notcars_test_small = []\n",
    "\n",
    "for img in cars_train:\n",
    "    cars_train_small.append(cv2.resize(img, (32,32)))\n",
    "for img in notcars_train:\n",
    "    notcars_train_small.append(cv2.resize(img, (32,32)))  \n",
    "train = np.append(cars_train_small, notcars_train_small, axis=0)\n",
    "train_normalize = train / 255 * 0.8 + 0.1\n",
    "train_lable = np.hstack((np.ones(len(cars_train)), np.zeros(len(notcars_train))))\n",
    "\n",
    "for img in cars_test:\n",
    "    cars_test_small.append(cv2.resize(img, (32,32)))\n",
    "for img in notcars_test:\n",
    "    notcars_test_small.append(cv2.resize(img, (32,32)))  \n",
    "test = np.append(cars_test_small, notcars_test_small, axis=0)\n",
    "test_normalize = test / 255 * 0.8 + 0.1\n",
    "test_lable = np.hstack((np.ones(len(cars_test)), np.zeros(len(notcars_test)))) \n",
    "                \n",
    "X_test, y_test = test_normalize, test_lable\n",
    "                       \n",
    "# Split up data into randomized training and validation sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train_normalize, train_lable, \n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=rand_state)\n",
    "# Summaries\n",
    "print(\"Number of training examples =\", len(X_train), len(y_train)) \n",
    "print(\"Number of validation examples =\", len(X_validation), len(y_validation)) \n",
    "print(\"Number of testing examples =\", len(X_test), len(y_test))                                               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Features and labels\n",
    "x = tf.placeholder(tf.float32, (None, 32, 32, 3))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 2)\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 32\n",
    "mu = 0\n",
    "sigma = 0.1\n",
    "strides = 1\n",
    "k = 2\n",
    "rate = 0.001\n",
    "\n",
    "# Implement LeNet-5\n",
    "def conv2d(x, W, b, strides):\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='VALID')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x, k):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='VALID')\n",
    "\n",
    "def LeNet(x):        \n",
    "    conv1 = conv2d(x, tf.Variable(tf.truncated_normal(shape=(5, 5, 3, 6), mean = mu, stddev = sigma)), \n",
    "                   tf.Variable(tf.zeros(6)), strides)\n",
    "    conv1 = maxpool2d(conv1, k)\n",
    "    conv2 = conv2d(conv1, tf.Variable(tf.truncated_normal(shape=(5, 5, 6, 16), mean = mu, stddev = sigma)),\n",
    "                   tf.Variable(tf.zeros(16)), strides)\n",
    "    conv2 = maxpool2d(conv2, k)    \n",
    "    fc11 = flatten(conv2)\n",
    "    fc11 = tf.add(tf.matmul(fc11, tf.Variable(tf.truncated_normal(shape=(400, 120), mean = mu, stddev = sigma))),\n",
    "                            tf.Variable(tf.zeros(120)))\n",
    "    fc11 = tf.nn.relu(fc11)\n",
    "    fc12 = tf.add(tf.matmul(fc11, tf.Variable(tf.truncated_normal(shape=(120, 32), mean = mu, stddev = sigma))),\n",
    "                            tf.Variable(tf.zeros(32)))\n",
    "    fc12 = tf.nn.relu(fc12)\n",
    "    out = tf.add(tf.matmul(fc12, tf.Variable(tf.truncated_normal(shape=(32, 2), mean = mu, stddev = sigma))),\n",
    "                           tf.Variable(tf.zeros(2)))\n",
    "    return out\n",
    "\n",
    "# Training Pipeline\n",
    "logits = LeNet(x)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits, one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "# Model Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Model Evaluation\n",
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy = 0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    train_accuracy = []\n",
    "    validation_accuracy = []\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        X_data, y_data = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_data[offset:end], y_data[offset:end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y})\n",
    "            \n",
    "        train_accuracy.append(evaluate(X_train, y_train))\n",
    "        validation_accuracy.append(evaluate(X_validation, y_validation))\n",
    "        \n",
    "    saver.save(sess, 'LeNet')\n",
    "    print(\"Model saved\") \n",
    "\n",
    "    plt.plot(range(EPOCHS), train_accuracy, 'r', label='Training Epochs Accuracy')\n",
    "    plt.plot(range(EPOCHS), validation_accuracy, 'x', label='Validation Epochs Accuracy')\n",
    "    plt.legend(loc=4)\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate the Model on test data\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, tf.train.latest_checkpoint('.'))\n",
    "    test_accuracy = evaluate(X_test, y_test)\n",
    "    print(\"Test Accuracy of LeNet = {:.5f}\".format(test_accuracy))\n",
    "\n",
    "del train, cars_train_small, notcars_train_small, test, cars_test_small, notcars_test_small, X_train, X_validation, y_train, y_validation, X_test, y_test, train_normalize, test_normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d. Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "test = []\n",
    " \n",
    "train = np.append(cars_train, notcars_train, axis=0)\n",
    "train_lable = np.hstack((np.ones(len(cars_train)), np.zeros(len(notcars_train))))\n",
    " \n",
    "test = np.append(cars_test, notcars_test, axis=0)\n",
    "test_lable = np.hstack((np.ones(len(cars_test)), np.zeros(len(notcars_test)))) \n",
    "                \n",
    "X_test, y_test = test, test_lable\n",
    "                       \n",
    "# Split up data into randomized training and validation sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(train, train_lable, \n",
    "                                                                test_size=0.2, \n",
    "                                                                random_state=rand_state)\n",
    "# Summaries\n",
    "print(\"Number of training examples =\", len(X_train), len(y_train)) \n",
    "print(\"Number of validation examples =\", len(X_validation), len(y_validation)) \n",
    "print(\"Number of testing examples =\", len(X_test), len(y_test))                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model specs are from https://github.com/HTuennermann/Vehicle-Detection-and-Tracking/blob/master/LocalizationModel.ipynb\n",
    "def get_conv(input_shape=(64,64,3), filename=None):\n",
    "    model = Sequential()\n",
    "    model.add(Lambda(lambda x: x/127.5 - 1.,input_shape=input_shape, output_shape=input_shape))\n",
    "    model.add(Convolution2D(10, 3, 3, activation='relu', name='conv1',input_shape=input_shape, border_mode=\"same\"))\n",
    "    model.add(Convolution2D(10, 3, 3, activation='relu', name='conv2',border_mode=\"same\"))\n",
    "    model.add(MaxPooling2D(pool_size=(8,8)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Convolution2D(128,8,8,activation=\"relu\",name=\"dense1\")) \n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Convolution2D(1,1,1,name=\"dense2\", activation=\"tanh\"))\n",
    "    if filename:\n",
    "        model.load_weights(filename)        \n",
    "    return model\n",
    "\n",
    "model = get_conv()\n",
    "model.add(Flatten())\n",
    "adam = Adam(lr=.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(optimizer=\"adam\", loss=\"mse\", metrics=['accuracy'])\n",
    "#model.compile(loss='mse',optimizer='adadelta',metrics=['accuracy'])\n",
    "model.fit(X_train, y_train, batch_size=128, nb_epoch=20, verbose=1, validation_data=(X_validation, y_validation))\n",
    "score = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "model.save_weights(\"Keras.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step - 3: Sliding Window Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Search and classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)\n",
    "\n",
    "def slide_window(img, x_start_stop=[None, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_windows = np.int(xspan/nx_pix_per_step) - 1\n",
    "    ny_windows = np.int(yspan/ny_pix_per_step) - 1\n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]           \n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64), interpolation=cv2.INTER_AREA)             \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = clf.predict(test_features)\n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the functions \n",
    "colorspace = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "spatial = 32\n",
    "histbin = 32\n",
    "orient = 9\n",
    "pix_per_cell = 8\n",
    "cell_per_block = 2\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "y_start_stop = [380, 660] # Min and max in y to search in slide_window()\n",
    "\n",
    "for i in range(1, 7):\n",
    "    img = cv2.imread('test_images/test' + str(i) + '.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    draw_image = np.copy(img)\n",
    "\n",
    "    windows = slide_window(img, x_start_stop=[None, None], y_start_stop=y_start_stop, \n",
    "                           xy_window=(96, 96), xy_overlap=(0.8, 0.8))\n",
    "\n",
    "    hot_windows = search_windows(img, windows, clf, X_scaler, color_space=colorspace, \n",
    "                                 spatial_size=(spatial, spatial), hist_bins=histbin, \n",
    "                                 orient=orient, pix_per_cell=pix_per_cell, \n",
    "                                 cell_per_block=cell_per_block, \n",
    "                                 hog_channel=hog_channel)                       \n",
    "\n",
    "    window_img = draw_boxes(draw_image, hot_windows, color=(0, 0, 255), thick=6)                    \n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.imshow(window_img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Hog sub-sampling window search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cars(img, ystart, ystop, scale, clf, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins):    \n",
    "    draw_img = np.copy(img)\n",
    "    \n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = cv2.cvtColor(img_tosearch, cv2.COLOR_RGB2YCrCb)\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell)-1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell)-1 \n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            test_prediction = clf.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                cv2.rectangle(draw_img,(xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart),(0,0,255),6) \n",
    "                \n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check the functions \n",
    "ystart = 400\n",
    "ystop = 656\n",
    "scale = 1.25\n",
    "\n",
    "# Read in test images\n",
    "for i in range(1, 7):\n",
    "    img = cv2.imread('test_images/test' + str(i) + '.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    out_img = find_cars(img, ystart, ystop, scale, clf, X_scaler, orient, pix_per_cell, \n",
    "                    cell_per_block, spatial_size=(32,32), hist_bins=32)                   \n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.imshow(out_img)\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Multiple detections & false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_cars(img, ystart, ystop, scale, clf, X_scaler, orient, pix_per_cell, cell_per_block, spatial_size, hist_bins):    \n",
    "    draw_img = np.copy(img)\n",
    "    \n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = cv2.cvtColor(img_tosearch, cv2.COLOR_RGB2YCrCb)\n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        \n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell)-1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell)-1 \n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    heat = np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            spatial_features = bin_spatial(subimg, size=spatial_size)\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            test_prediction = clf.predict(test_features)\n",
    "          \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                heat[(ytop_draw+ystart):(ytop_draw+win_draw+ystart), xbox_left:(xbox_left+win_draw)] += 1\n",
    "\n",
    "    heat[heat <= 1] = 0            \n",
    "#    heatmap = np.clip(heat, 0, 255)\n",
    "    labels = label(heat)\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        w = np.max(nonzerox)-np.min(nonzerox)\n",
    "        h = np.max(nonzeroy)-np.min(nonzeroy)\n",
    "        #if (75 < w < 250) & (25 < h < 150):\n",
    "        cv2.rectangle(draw_img, bbox[0], bbox[1], (0,0,255), 6) \n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ystart = 400\n",
    "ystop = 656\n",
    "\n",
    "# Read in test images\n",
    "for i in range(1, 7):\n",
    "    img = cv2.imread('test_images/test' + str(i) + '.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    out_img = find_cars(img, ystart, ystop, scale, clf, X_scaler, orient, pix_per_cell, \n",
    "                    cell_per_block, spatial_size=(32,32), hist_bins=32)                   \n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.imshow(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step - 4: Video Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Sliding window + classifier approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def color_hist(img, nbins=32, bins_range=(0, 256)):\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins, range=bins_range)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins, range=bins_range)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins, range=bins_range)\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    return hist_features\n",
    "\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, feature_vec=True):   \n",
    "    features = hog(img, orientations=orient, pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                    cells_per_block=(cell_per_block, cell_per_block), transform_sqrt=True, \n",
    "                    visualise=False, feature_vector=feature_vec)\n",
    "    return features\n",
    "\n",
    "prev_hot_windows1 = []\n",
    "prev_hot_windows2 = []\n",
    "prev_hot_windows3 = []\n",
    "\n",
    "def process_video1(img, ystart=400, ystop=656, clf=clf, X_scaler=X_scaler, \n",
    "                  orient=9, pix_per_cell=8, cell_per_block=2, spatial_size=(32, 32), \n",
    "                  hist_bins=32):    \n",
    "    global prev_hot_windows1, prev_hot_windows2, prev_hot_windows3\n",
    "    \n",
    "    ctrans_tosearch = cv2.cvtColor(img[ystart:ystop,:,:], cv2.COLOR_RGB2YCrCb)\n",
    "\n",
    "    ch1 = ctrans_tosearch[:,:,0]\n",
    "    ch2 = ctrans_tosearch[:,:,1]\n",
    "    ch3 = ctrans_tosearch[:,:,2]\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    nxblocks = (ch1.shape[1] // pix_per_cell)-1\n",
    "    nyblocks = (ch1.shape[0] // pix_per_cell)-1 \n",
    "    nfeat_per_block = orient*cell_per_block**2\n",
    "\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell)-1 \n",
    "    cells_per_step = 2  \n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step\n",
    "    \n",
    "    # Compute individual channel HOG features for the entire image\n",
    "    hog1 = get_hog_features(ch1, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog2 = get_hog_features(ch2, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    hog3 = get_hog_features(ch3, orient, pix_per_cell, cell_per_block, feature_vec=False)\n",
    "    \n",
    "    heat = np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "    \n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            # Extract HOG for this patch\n",
    "            hog_feat1 = hog1[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat2 = hog2[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_feat3 = hog3[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel() \n",
    "            hog_features = np.hstack((hog_feat1, hog_feat2, hog_feat3))\n",
    "\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "\n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(ctrans_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "          \n",
    "            # Get color features\n",
    "            spatial_features = cv2.resize(subimg, spatial_size).ravel()\n",
    "            hist_features = color_hist(subimg, nbins=hist_bins)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            test_features = X_scaler.transform(np.hstack((spatial_features, hist_features, hog_features)).reshape(1, -1))    \n",
    "            test_prediction = clf.predict(test_features)\n",
    "          \n",
    "            if test_prediction == 1:\n",
    "                xbox_left = np.int(xleft)\n",
    "                ytop_draw = np.int(ytop)\n",
    "                win_draw = np.int(window)\n",
    "                heat[(ytop_draw+ystart):(ytop_draw+win_draw+ystart), xbox_left:(xbox_left+win_draw)] += 1\n",
    "\n",
    "    for bbox in prev_hot_windows1:\n",
    "        heat[(bbox[0][1]):(bbox[1][1]), (bbox[0][0]):(bbox[1][0])] += 0.8\n",
    "    for bbox in prev_hot_windows2:\n",
    "        heat[(bbox[0][1]):(bbox[1][1]), (bbox[0][0]):(bbox[1][0])] += 0.6   \n",
    "    for bbox in prev_hot_windows3:\n",
    "        heat[(bbox[0][1]):(bbox[1][1]), (bbox[0][0]):(bbox[1][0])] += 0.4    \n",
    "\n",
    "    heat[heat <= 5] = 0 \n",
    "\n",
    "    prev_hot_windows3 = list(prev_hot_windows2)\n",
    "    prev_hot_windows2 = list(prev_hot_windows1)\n",
    "    prev_hot_windows1 = []\n",
    "                    \n",
    "#    heatmap = np.clip(heat, 0, 255)\n",
    "    labels = label(heat)\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        w = np.max(nonzerox)-np.min(nonzerox)\n",
    "        h = np.max(nonzeroy)-np.min(nonzeroy)\n",
    "        if (75 < w < 250) & (25 < h < 150):           \n",
    "            cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6) \n",
    "            prev_hot_windows1.append(bbox)\n",
    "            \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Keras model approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = get_conv(input_shape=(None,None,3), filename=\"Keras_adam.h5\")\n",
    "prev_hot_windows1 = []\n",
    "prev_hot_windows2 = []\n",
    "prev_hot_windows3 = []\n",
    "\n",
    "def process_video2(img, ystart=400, ystop=656):   \n",
    "    global prev_hot_windows1, prev_hot_windows2, prev_hot_windows3    \n",
    "    pred = model.predict(img.reshape(1,img.shape[0],img.shape[1],img.shape[2]))\n",
    "\n",
    "    xx, yy = np.meshgrid(np.arange(pred.shape[2]),np.arange(pred.shape[1]))\n",
    "    box_x = (xx[pred[0,:,:,0]>0.99])\n",
    "    box_y = (yy[pred[0,:,:,0]>0.99])\n",
    "    \n",
    "    heat = np.zeros_like(img[:,:,0]).astype(np.float)\n",
    "\n",
    "    for x,y in zip(box_x, box_y):\n",
    "        if (y*8 > 375) & (x*8 > 500):\n",
    "            x1 = x*8\n",
    "            x2 = x*8 + 70\n",
    "            y1 = y*8\n",
    "            y2 = y*8 + 70\n",
    "            heat[(y1):(y2), (x1):(x2)] += 1\n",
    "            \n",
    "    for bbox in prev_hot_windows1:\n",
    "        heat[(bbox[0][1]):(bbox[1][1]), (bbox[0][0]):(bbox[1][0])] += 4\n",
    "    for bbox in prev_hot_windows2:\n",
    "        heat[(bbox[0][1]):(bbox[1][1]), (bbox[0][0]):(bbox[1][0])] += 3   \n",
    "    for bbox in prev_hot_windows3:\n",
    "        heat[(bbox[0][1]):(bbox[1][1]), (bbox[0][0]):(bbox[1][0])] += 2          \n",
    "            \n",
    "    heat[heat <= 9] = 0\n",
    "    \n",
    "    prev_hot_windows3 = list(prev_hot_windows2)\n",
    "    prev_hot_windows2 = list(prev_hot_windows1)\n",
    "    prev_hot_windows1 = []\n",
    "    \n",
    "    labels = label(heat)\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        w = np.max(nonzerox)-np.min(nonzerox)\n",
    "        h = np.max(nonzeroy)-np.min(nonzeroy)\n",
    "        if (50 < w < 400) & (25 < h < 250):\n",
    "            cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6) \n",
    "            prev_hot_windows1.append(bbox)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1, 7):\n",
    "    img = cv2.imread('test_images/test' + str(i) + '.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    prev_hot_windows1 = []\n",
    "    prev_hot_windows2 = []\n",
    "    prev_hot_windows3 = []\n",
    "    out_img = process_video2(img, ystart=400, ystop=656)                  \n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(3, 2, i)\n",
    "    plt.imshow(out_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Video!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_output = 'test_video_output.mp4'\n",
    "clip1 = VideoFileClip('test_video.mp4')\n",
    "test_clip = clip1.fl_image(process_video2) \n",
    "%time test_clip.write_videofile(test_output, audio = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(test_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_output = 'project_video_output_p5.mp4'\n",
    "clip1 = VideoFileClip('project_video_output_p4.mp4')\n",
    "project_clip = clip1.fl_image(process_video2) \n",
    "%time project_clip.write_videofile(project_output, audio = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(project_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
